1.Link to configure AWS RDS in private network and connect to it from outside of VPC - https://www.linkedin.com/pulse/connecting-rds-db-private-subnet-aws-from-using-ssm-port-srinivasan?trk=pulse-article_more-articles_related-content-card
follow option 3 for AWS RDS  Aurora postgres/mysql

2.JWT authentication with nginx as reverse proxy https://www.nginx.com/blog/authenticating-api-clients-jwt-nginx-plus/

3.Create a frontend service in public subnet with all the ec2 instances in public network, whilst put backend dotnet api cluster/services into the private network in the same vpc and then send requests from frontend(service1) to backend(service2) using service2's public ip address or we if there's a load balancer sitting in front of it then replace it with the ip address of service2's load balancer's ip address (https://aws.amazon.com/blogs/aws/amazon-ecs-service-discovery/) 

4.Service discovery tutorial - https://devops4solutions.medium.com/deploy-java-microservices-using-aws-ecs-service-discovery-27a8ba71dbd4

5.In service discovery we register services in route53 hosted zone, through which one service can make requests to another service 
with just the service name as route53 dns provider will resolve service name to the ip addresses of the running containers, so we don't even have to use load balancer in front of any service, other than if we're hosting multple ec2 instances
  

Unsplash api credentials - 
username - abhishek_abhishek
access_key - 8p0X14fHnptNmCnBUaS6uc0dpWFxrXoDM-DeDkcLB7c
secret_key - cbi3tQuEv62oarj2eAQslhf95avFSesQASiCYHsuutI
sample curl request - 
curl --location --request GET 'https://api.unsplash.com/search/photos?client_id=8p0X14fHnptNmCnBUaS6uc0dpWFxrXoDM-DeDkcLB7c&query=books'
replace the "query=<value>" with type of photos we want, and also iterate over all pages ?page=1 till "total_pages": <>,
sample response - 
{
 total: total number of images
 "total_pages": <>,
  result: []
}

documention - https://unsplash.com/documentation#search-photos
then write each file to folder and save it and upload it to s3 bucket as a zip file/ or directely write it to s3 bucket 


we can replace 
COPY ./start-up.sh /docker-entrypoint-initdb.d/
RUN chmod +x /docker-entrypoint-initdb.d/start-up.sh
with
COPY .init.sql /docker-entrypoint-initdb.d/
and then
GRANT ALL ON ALL TABLES IN SCHEMA public TO dbowner;
but the above didn't worked on line 8 of init.sql

we can follow the steps below to fix this
steps - 
1. go to postgres container
2. login as super user i.e pssql -U posgres <db-name>
3. run this command GRANT ALL ON ALL TABLES IN SCHEMA public TO <role-name>;
4. login as <user> to which the above role is attached to 
5. now you'll be able to perform crud operations

# inside init.sql
GRANT ALL PRIVILEGES ON DATABASE dotndockdb TO abhishek; 
ALTER DATABASE dotndockdb OWNER TO dbowner; 
the above two didn't worked as  
they will make this user temporary owner with only create access no read access on any table

Add these to dockerfile or pass in via docker run command if we don't wanna create a user and database from initial script
ENV POSTGRES_USER abhishek
ENV POSTGRES_PASSWORD docker
ENV POSTGRES_DB docker

Postgres commands 
\d - list table relations
\l - list tables
\zl tablename - verify access levels on table
\du - list all the users
\c databasename - connect to database
\q - exit of psql
ctrl + shift + L - clear psql console

# Add following lines to the dockerfile of the dotnet-core image, and also add a /health endpoint either in program.cs if using 
minimal api or a new controller, which will check the connection with database etc. 
On how to implement health check (https://code-maze.com/health-checks-aspnetcore/), (https://learn.microsoft.com/en-us/aspnet/core/host-and-deploy/health-checks?view=aspnetcore-6.0)
RUN apt-get update 
RUN apt-get --yes install curl
HEALTHCHECK --interval=5s --timeout=10s --retries=3 CMD curl --silent --fail http://localhost/health || exit 1
If we have multiple health endpoints we can use a script that makes multiple curl requests like below
COPY files/healthcheck.sh /healthcheck.sh
RUN chmod +x /healthcheck.sh
HEALTHCHECK --interval=60s --timeout=10s --start-period=10s \  
    CMD /healthcheck.sh || exit 1
That way you can do multiple requests via curl and let the script only return positive, if all requests succeeded.
we can use a sample script like this (https://stackoverflow.com/questions/37885025/rest-service-health-check-on-automatic-deployment)

We can utilise similar architecture for authentication/authorization using api gateway before we pass requests to the ECS cluster 
(https://aws.amazon.com/blogs/security/building-fine-grained-authorization-using-amazon-cognito-api-gateway-and-iam/)
(https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-the-id-token.html)
(https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-verifying-a-jwt.html)
(https://github.com/awslabs/aws-support-tools/tree/master/Cognito/decode-verify-jwt)
(https://aws.amazon.com/blogs/containers/securing-amazon-elastic-container-service-applications-using-application-load-balancer-and-amazon-cognito/)

# Approach for policy based auth for backend services using api-gateway, ecs and cognito 
1. Running Backend tasks in private subnet of vpc
2. Running Api gateway in public subnet of same VPC
3. Connect Api gateway using VPC link to the backend with network load balancer in place(or search if we can directly connect api gaetway to backend service in private subnet using vpc link without requiring NLB, as it is costly)
4. Set up Cognito user pools
5. When a user hits the the /get/pets endpoint it'll pass that request to lambda authorizer with the token from the cognito and then it'll get policy from dynamodb, returns it back to the api gateway which then caches that policy for the same user for further requests, and then pass that onto the network load balancer which'll then forward to the backend service.
6 The above steps assume that the user already exists in cognito user pool

# Approch for frontend auth using cognito, ecs, and ALB
1.A user sends a request to the application fronted by the ALB, which has a set of rules that it evaluates for all traffic to determine what action to carry out. The rule (such as the path-based rule saying all traffic for/login) when matched triggers the authentication action on the ALB. The ALB then inspects the user’s HTTP payload for an authentication cookie.
2.Because this is the user’s first visit, this cookie isn’t present. The ALB doesn’t see any cookie and redirects the user to the configured Amazon Cognito’s authorization endpoint.
3.The user is presented with an authentication page from Amazon Cognito, where the user inputs their credentials. Amazon Cognito redirects the user back to the ALB and passes an authorization code to the user in the redirect URL.
4.The load balancer takes this authorization code and makes a request to Amazon Cognito’s token endpoint.
5.Amazon Cognito validates the authorization code and presents the ALB with an ID and access token.
6.The ALB forwards the access token to Amazon Cognito’s user info endpoint.
7.Amazon Cognito’s user information endpoint presents the ALB with user claims.
8.The ALB redirects the user who is trying to access the application (step 1) to the same URL while inserting the authentication cookie in the redirect response.
9.The user makes the request to the ALB with the cookie and the ALB validates it and forwards the request to the ALB’s target. The 10.ALB inserts information (such as user claims, access token, and the subject field) into a set of X-AMZN-OIDC-* HTTP headers to the target.
11.The target generates a response and forwards to the ALB.
12.The ALB sends the response to the authenticated user.
When the user makes subsequent requests for HTTP request and response, the flow will go through steps 9–11. If the user makes a new request without the authentication cookie, it goes through steps 1–11.

------
.We can use angular route resolver to pre load the data before component gets rendered
2.For components that are lazy loaded(means it is only loaded into memory when we need it), we add CommonModule, Router.ForChild(childroutes) inside imports property, rather than BrowserModule and Router.ForRoute as we do inside app module, we can achieve this using dynamic import
3.We can use <ng-content></ng-content> to get the child component wrappend within the parent component, it is similar to react's this.children
4.we can also use <ng-content selector="seleect by class, select by attribute"> to display only selected component for example - 

child component - 
<div class="section1">
 <p>section 1</p> 
<div>
<div class="section2">
 <p>section 2</p> 
<div>

Then inside parent component 
<parent>
 <child></child>
</parent>

parent component - 
 <div>
	<h5> Parent component /<h5>
	<div *ngIf="somecondition">
          <ng-content selector=".section1"></ng-content>
	</div> 
        <div *ngIf="someothercondition">
          <ng-content selector=".section2"></ng-content>
	</div> 
 </div>

This will display different parts of child component based on somecontion using any kind of selector, it is called content projection
5.We can InjectionToken rather registering a class for DI
6.We can use #whatevername to reference a component and then using @ViewChild to get the actual dom element
For example
 <div class="trash" #mycomponent>
	<h5> Parent component /<h5>
 </div>

@ViewChild('#mycomponent') myreference: ElementRef;
const div = this.myreference.nativeElement; 

To get dom elements of the child comoponent as from step 4 we have to use @ViewContent


(https://www.baeldung.com/linux/use-command-line-arguments-in-bash-script)
