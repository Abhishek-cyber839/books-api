1.Link to configure AWS RDS in private network and connect to it from outside of VPC - https://www.linkedin.com/pulse/connecting-rds-db-private-subnet-aws-from-using-ssm-port-srinivasan?trk=pulse-article_more-articles_related-content-card
follow option 3 for AWS RDS  Aurora postgres/mysql

2.JWT authentication with nginx as reverse proxy https://www.nginx.com/blog/authenticating-api-clients-jwt-nginx-plus/

3.Create a frontend service in public subnet with all the ec2 instances in public network, whilst put backend dotnet api cluster/services into the private network in the same vpc and then send requests from frontend(service1) to backend(service2) using service2's public ip address or we if there's a load balancer sitting in front of it then replace it with the ip address of service2's load balancer's ip address (https://aws.amazon.com/blogs/aws/amazon-ecs-service-discovery/) 

4.Service discovery tutorial - https://devops4solutions.medium.com/deploy-java-microservices-using-aws-ecs-service-discovery-27a8ba71dbd4

5.In service discovery we register services in route53 hosted zone, through which one service can make requests to another service 
with just the service name as route53 dns provider will resolve service name to the ip addresses of the running containers, so we don't even have to use load balancer in front of any service, other than if we're hosting multple ec2 instances
  

Unsplash api credentials - 
username - abhishek_abhishek
access_key - 8p0X14fHnptNmCnBUaS6uc0dpWFxrXoDM-DeDkcLB7c
secret_key - cbi3tQuEv62oarj2eAQslhf95avFSesQASiCYHsuutI
sample curl request - 
curl --location --request GET 'https://api.unsplash.com/search/photos?client_id=8p0X14fHnptNmCnBUaS6uc0dpWFxrXoDM-DeDkcLB7c&query=books'
replace the "query=<value>" with type of photos we want, and also iterate over all pages ?page=1 till "total_pages": <>,
sample response - 
{
 total: total number of images
 "total_pages": <>,
  result: []
}

documention - https://unsplash.com/documentation#search-photos
then write each file to folder and save it and upload it to s3 bucket as a zip file/ or directely write it to s3 bucket 


we can replace 
COPY ./start-up.sh /docker-entrypoint-initdb.d/
RUN chmod +x /docker-entrypoint-initdb.d/start-up.sh
with
COPY .init.sql /docker-entrypoint-initdb.d/
and then
GRANT ALL ON ALL TABLES IN SCHEMA public TO dbowner;
but the above didn't worked on line 8 of init.sql

we can follow the steps below to fix this
steps - 
1. go to postgres container
2. login as super user i.e pssql -U posgres <db-name>
3. run this command GRANT ALL ON ALL TABLES IN SCHEMA public TO <role-name>;
4. login as <user> to which the above role is attached to 
5. now you'll be able to perform crud operations

# inside init.sql
GRANT ALL PRIVILEGES ON DATABASE dotndockdb TO abhishek; 
ALTER DATABASE dotndockdb OWNER TO dbowner; 
the above two didn't worked as  
they will make this user temporary owner with only create access no read access on any table

Add these to dockerfile or pass in via docker run command if we don't wanna create a user and database from initial script
ENV POSTGRES_USER abhishek
ENV POSTGRES_PASSWORD docker
ENV POSTGRES_DB docker

Postgres commands 
\d - list table relations
\l - list tables
\zl tablename - verify access levels on table
\du - list all the users
\c databasename - connect to database
\q - exit of psql
ctrl + shift + L - clear psql console
