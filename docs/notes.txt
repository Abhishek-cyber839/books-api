1.Link to configure AWS RDS in private network and connect to it from outside of VPC - https://www.linkedin.com/pulse/connecting-rds-db-private-subnet-aws-from-using-ssm-port-srinivasan?trk=pulse-article_more-articles_related-content-card
follow option 3 for AWS RDS  Aurora postgres/mysql

2.JWT authentication with nginx as reverse proxy https://www.nginx.com/blog/authenticating-api-clients-jwt-nginx-plus/

3.Create a frontend service in public subnet with all the ec2 instances in public network, whilst put backend dotnet api cluster/services into the private network in the same vpc and then send requests
 from frontend(service1) to backend(service2) using service2's public ip address or we if there's a load balancer sitting in front of it then replace it with the ip address of service2's load balancer's ip address (https://aws.amazon.com/blogs/aws/amazon-ecs-service-discovery/) 

4.Service discovery tutorial - https://devops4solutions.medium.com/deploy-java-microservices-using-aws-ecs-service-discovery-27a8ba71dbd4

5.In service discovery we register services in route53 hosted zone, through which one service can make requests to another service 
with just the service name as route53 dns provider will resolve service name to the ip addresses of the running containers, so we don't even have to use load balancer in front of any service, other than if we're hosting multple ec2 instances
  

Unsplash api credentials - 
username - abhishek_abhishek
access_key - 8p0X14fHnptNmCnBUaS6uc0dpWFxrXoDM-DeDkcLB7c
secret_key - cbi3tQuEv62oarj2eAQslhf95avFSesQASiCYHsuutI
sample curl request - 
curl --location --request GET 'https://api.unsplash.com/search/photos?client_id=8p0X14fHnptNmCnBUaS6uc0dpWFxrXoDM-DeDkcLB7c&query=books'
replace the "query=<value>" with type of photos we want, and also iterate over all pages ?page=1 till "total_pages": <>,
sample response - 
{
 total: total number of images
 "total_pages": <>,
  result: []
}

documention - https://unsplash.com/documentation#search-photos
then write each file to folder and save it and upload it to s3 bucket as a zip file/ or directely write it to s3 bucket 


we can replace 
COPY ./start-up.sh /docker-entrypoint-initdb.d/
RUN chmod +x /docker-entrypoint-initdb.d/start-up.sh
with
COPY .init.sql /docker-entrypoint-initdb.d/
and then
GRANT ALL ON ALL TABLES IN SCHEMA public TO dbowner;
but the above didn't worked on line 8 of init.sql

we can follow the steps below to fix this
steps - 
1. go to postgres container
2. login as super user i.e pssql -U posgres <db-name>
3. run this command GRANT ALL ON ALL TABLES IN SCHEMA public TO <role-name>;
4. login as <user> to which the above role is attached to 
5. now you'll be able to perform crud operations

# inside init.sql
GRANT ALL PRIVILEGES ON DATABASE dotndockdb TO abhishek; 
ALTER DATABASE dotndockdb OWNER TO dbowner; 
the above two didn't worked as  
they will make this user temporary owner with only create access no read access on any table

Add these to dockerfile or pass in via docker run command if we don't wanna create a user and database from initial script
ENV POSTGRES_USER abhishek
ENV POSTGRES_PASSWORD docker
ENV POSTGRES_DB docker

Postgres commands 
\d - list table relations
\l - list tables
\zl tablename - verify access levels on table
\du - list all the users
\c databasename - connect to database
\q - exit of psql
ctrl + shift + L - clear psql console

# Add following lines to the dockerfile of the dotnet-core image, and also add a /health endpoint either in program.cs if using 
minimal api or a new controller, which will check the connection with database etc. 
On how to implement health check (https://code-maze.com/health-checks-aspnetcore/), (https://learn.microsoft.com/en-us/aspnet/core/host-and-deploy/health-checks?view=aspnetcore-6.0)
RUN apt-get update 
RUN apt-get --yes install curl
HEALTHCHECK --interval=5s --timeout=10s --retries=3 CMD curl --silent --fail http://localhost/health || exit 1
If we have multiple health endpoints we can use a script that makes multiple curl requests like below
COPY files/healthcheck.sh /healthcheck.sh
RUN chmod +x /healthcheck.sh
HEALTHCHECK --interval=60s --timeout=10s --start-period=10s \  
    CMD /healthcheck.sh || exit 1
That way you can do multiple requests via curl and let the script only return positive, if all requests succeeded.
we can use a sample script like this (https://stackoverflow.com/questions/37885025/rest-service-health-check-on-automatic-deployment)

We can utilise similar architecture for authentication/authorization using api gateway before we pass requests to the ECS cluster 
(https://aws.amazon.com/blogs/security/building-fine-grained-authorization-using-amazon-cognito-api-gateway-and-iam/)
(https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-the-id-token.html)
(https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-verifying-a-jwt.html)
(https://github.com/awslabs/aws-support-tools/tree/master/Cognito/decode-verify-jwt)
(https://aws.amazon.com/blogs/containers/securing-amazon-elastic-container-service-applications-using-application-load-balancer-and-amazon-cognito/)

# Approach for policy based auth for backend services using api-gateway, ecs and cognito 
1. Running Backend tasks in private subnet of vpc
2. Running Api gateway in public subnet of same VPC
3. Connect Api gateway using VPC link to the backend with network load balancer in place(or search if we can directly connect api gaetway to backend service in private subnet using vpc link without requiring NLB, as it is costly)
4. Set up Cognito user pools
5. When a user hits the the /get/pets endpoint it'll pass that request to lambda authorizer with the token from the cognito and then it'll get policy from dynamodb, returns it back to the api gateway which then caches that policy for the same user for further requests, and then pass that onto the network load balancer which'll then forward to the backend service.
6 The above steps assume that the user already exists in cognito user pool

# Approch for frontend auth using cognito, ecs, and ALB
1.A user sends a request to the application fronted by the ALB, which has a set of rules that it evaluates for all traffic to determine what action to carry out. The rule (such as the path-based rule saying all traffic for/login) when matched triggers the authentication action on the ALB. The ALB then inspects the user’s HTTP payload for an authentication cookie.
2.Because this is the user’s first visit, this cookie isn’t present. The ALB doesn’t see any cookie and redirects the user to the configured Amazon Cognito’s authorization endpoint.
3.The user is presented with an authentication page from Amazon Cognito, where the user inputs their credentials. Amazon Cognito redirects the user back to the ALB and passes an authorization code to the user in the redirect URL.
4.The load balancer takes this authorization code and makes a request to Amazon Cognito’s token endpoint.
5.Amazon Cognito validates the authorization code and presents the ALB with an ID and access token.
6.The ALB forwards the access token to Amazon Cognito’s user info endpoint.
7.Amazon Cognito’s user information endpoint presents the ALB with user claims.
8.The ALB redirects the user who is trying to access the application (step 1) to the same URL while inserting the authentication cookie in the redirect response.
9.The user makes the request to the ALB with the cookie and the ALB validates it and forwards the request to the ALB’s target. The 10.ALB inserts information (such as user claims, access token, and the subject field) into a set of X-AMZN-OIDC-* HTTP headers to the target.
11.The target generates a response and forwards to the ALB.
12.The ALB sends the response to the authenticated user.
When the user makes subsequent requests for HTTP request and response, the flow will go through steps 9–11. If the user makes a new request without the authentication cookie, it goes through steps 1–11.

# Use both fargate and ec2 launch type along with path based routing, we can basically move the entire architecure into private even the load balancer, and then create an api-gateway on public internet which will connect to the load balancer throught vpc link
(https://aws.amazon.com/blogs/containers/using-aws-application-load-balancer-path-based-routing-to-combine-amazon-ecs-launch-types/)
:- Do note that with fargate launch type we cannot access host instance's volume, i.e no persistence volume to overcome this we have to use EFS (https://aws.amazon.com/premiumsupport/knowledge-center/ecs-fargate-mount-efs-containers-tasks/) 
:- If we wanna access efs from our local pc we can create an ec2 in public subnet and efs in public/private depending on our need, then create a directory in efs and in ec2 for a user for ex - /efs/demo-user and mount that to efs, then use whatever directory you want in your local pc to mount to that /efs/local -> /efs/demo-user and make changes or access
 files using sshfs which gives us access(mounts local directories to remote servers) to the whole filesystem using ssh, the end solution will be 
/efs/local -> /efs/demo-user -> efs/efs
(https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh)

# We can use codebuild to run unit tests and report failure/success to github's api which will then compare the status of the current commit and based on that it will build the image and push it to ECR, change the task definition, make the service to deploy new containers in the ecs cluster, in the below example we can replace it with Xunit, moq tests of dotnet 
(https://aws.amazon.com/blogs/containers/create-a-ci-cd-pipeline-for-amazon-ecs-with-github-actions-and-aws-codebuild-tests/)
:- As dotnet 6 is not yet supported with codebuild so we can test it in github action using one 
(https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-net)
(https://docs.github.com/en/actions/deployment/deploying-to-your-cloud-provider/deploying-to-amazon-elastic-container-service)
(https://docs.github.com/en/actions/using-workflows/storing-workflow-data-as-artifacts) - to chek for the failure tests, we can also upload artifacts to s3 bucket
:- Make these changes in the actions.yaml file that we'll generate from the above 3 links
 - name: Build app
    id: build
    run: |
      <build command>

 - name: Run tests only when build passes
   id: test
   run: |
      <test command>

 - name: Build , tag and push new image to ecr only when test passes
   if: steps.test.outcome == 'success'

:- As from dotnet 6 onwards program.cs file is internal means we cannot acces it outside of the project so add the below to the .csproj file in order to access this in the test project
 Make the Program class public using a partial class declaration:
 public partial class Program { } and configure everything inside it
 <ItemGroup>
  <InternalsVisibleTo Include="$test_project_name" />
  </ItemGroup>
  later inside the test project within the test class add the following line 
  var webAppFactory = new WebApplicationFactory<Program>(); 
  as web application factory will run the api itself we don't have to use dotnet run <api_directory> in order to test our web api

  we can register our own in memory test database by creating out CustomWebApplicationFactory like below 
  public class CustomWebApplicationFactory<T>
    : WebApplicationFactory<TStartup> where T: class 
  with older versions we use to pass startup so we can also use the above
  services.AddDbContext<ApplicationDbContext>(options =>
   {
                options.UseInMemoryDatabase("InMemoryDbForTesting");
   });
   
   reference - https://dotnetthoughts.net/dotnet-minimal-api-integration-testing/

:- To share same instance of class among different test classes use ICollectionFixture<DatabaseFixture>
https://xunit.net/docs/shared-context

:- We can use postgres container during the testing step as test database of the build job which is provided by the github actions for ex - 
in action.yaml file add the below content
env:
  POSTGRES_HOST: localhost
  POSTGRES_USER: postgres
  POSTGRES_PASSWORD: postgres
  POSTGRES_DB: <db_name>

jobs :
  build:
    runs-on: ubuntu-latest   
  # Service containers to run with `container-job`
    services:
      # Label used to access the service container
      postgres:
        # Docker Hub image
        image: postgres
        # Provide the password for postgres
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: <db_name>
        ports:
          # Maps tcp port 5432 on service container to the host
          - 5432:5432  
        # Set health checks to wait until postgres has started
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
        - name: Install PostgreSQL client
          run: |
          apt-get update
          apt-get install --yes postgresql-client
        # queries database with postgres client
        - name: Query database
          run: psql -h postgres -d <db_name> -U postgres -c 'SELECT 1;'
          env:
            POSTGRES_USER: postgres
            POSTGRES_PASSWORD: postgres
        - name: Setup database
          id: database_setup
          run: psql -h postgres -U postgres_user -c 'CREATE DATABASE "<db_name>";'
          env:
            POSTGRES_USER: postgres
            POSTGRES_PASSWORD: postgres
        # only build and test if the database is successfully set up for use
        - name: Build app
          if: steps.database_setup.outcome == 'success'
          id: build
          run: |
              <build command>
        - name: Run tests only when build passes # use above env variable as connectionstring for entityframework test 
          id: test
          run: |
              <test command>
        # only build new image when test passes for new changes      
        - name: Build , tag and push new image to ecr only when test passes
          if: steps.test.outcome == 'success' 

reference to create database for postgres container - 

If the above database_setup step fails we can use the below to run migration script to create and seed database and then replace 
the properties in the above action.yaml file and Xunit-EFcore.txt, and we can also remove all the seeding logic from TestDatabaseFixture()
as database will already be seeded and verified from the below steps
- name: Run migrations
  run: psql -f /books-api/postgres-script/init.sql postgresql://postgres:postgres@localhost:5432/postgres
- name: Query database to verify above script
  run: psql -h postgres -d <db_name> -U postgres -c 'SELECT 1;'
  env:
    POSTGRES_USER: postgres
    POSTGRES_PASSWORD: postgres
reference - https://catzkorn.dev/blog/postgres-github-actions/      
     

Then inside Test Project Create new folder and files and seperate content inside Xunit-EFcore.txt to seprate files, as it shows us
how we can use postgres database to test against production like environment rather than in memory database

# BUT FOR REPOSITORY PATTERNS MOCKING IS BEST PRACTICE RATHER THAN USING IN-MEMORY DATABASE, IT SHOULD ONLY BE USED IF WE'RE ACCESSING ENTITY FRAMEWORK'S CONTEXT OBJECT DIRECTLY FOR EX - _dbcontext.dbtable.getAsync() INSIDE THE REPOSITORY INSIDE THE CONTROLLER RATHER THAN CALLING A _myRepository.getAsync()

-------
# Monitoring application logs from ECS containers using datadog
 - https://docs.datadoghq.com/containers/amazon_ecs/?tab=awscli 
 - https://docs.datadoghq.com/containers/amazon_ecs/data_collected/
 - https://docs.datadoghq.com/infrastructure/containermap/
 - https://docs.datadoghq.com/continuous_integration/pipelines/github/
 - https://docs.datadoghq.com/integrations/ecs_fargate/?tab=webui
 - https://docs.datadoghq.com/tracing/glossary/
 - https://docs.datadoghq.com/tracing/trace_collection/dd_libraries/dotnet-core/?tab=linux
 
:- We can use database monitoring of datadog for testing postgres container in the ci/cd pipeline 
:- communication from vpc to datadog vpc over private link https://docs.datadoghq.com/agent/guide/private-link/?tab=useast1

# ECS fargate type monitoring using datadog
  - https://www.datadoghq.com/blog/aws-fargate-monitoring-with-datadog/
  - https://www.datadoghq.com/blog/aws-fargate-monitoring-with-datadog/#explore-all-your-ecs-and-eks-fargate-monitoring-data ( ECS     logs into Datadog using Fluent Bit)
  - https://www.datadoghq.com/blog/aws-fargate-monitoring-with-datadog/#the-container-map (birdeye view of containers)
  - https://www.datadoghq.com/blog/asp-dotnet-core-monitoring/ (dotnet with datadog locally)
  - https://www.datadoghq.com/blog/deploy-dotnet-core-aws-fargate/ (dotnet with datadog on ecs fargate launch type)
  - https://www.datadoghq.com/blog/monitoring-ecs-with-datadog/#from-ecs-to-datadog

# Installing data-dog agent
 1.DD_API_KEY=7dffb9eba72d4e1c514884ed2e3c7d44
 2.DD_SITE="us5.datadoghq.com" 
 3.bash -c "$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script_agent7.sh)"
 - If the Agent is not already installed on your machine and you don't want it to start automatically after the installation, just     prepend DD_INSTALL_ONLY=true to the above script before running it.

# Monitoring AWS RDS Aurora Postgres
 - https://docs.datadoghq.com/database_monitoring/setup_postgres/aurora/?tab=docker
 (host your rds in private subnet then follow the verification process of above link,  through an EC2 instance which you'll create
  in public subnet of the same VPC, as we can't connect from our local i.e outside of VPC to our rds, so we have to ssh into the   public ec2 and then connect through it to the rds, or we can also you session manager/SSM for this purpose)
 - https://www.datadoghq.com/blog/monitor-aurora-using-datadog/

# Monitoring GitHub Actions workflow
 - https://www.datadoghq.com/blog/datadog-github-actions-ci-visibility/

# AWS App Mesh 
 - https://docs.aws.amazon.com/app-mesh/latest/userguide/what-is-app-mesh.html
 - https://docs.aws.amazon.com/app-mesh/latest/userguide/envoy.html
 - https://docs.aws.amazon.com/app-mesh/latest/userguide/sharing.html (for communications between ecs services in different      accounts/vpc's use shared mesh)
 - https://aws.amazon.com/blogs/aws/amazon-ecs-service-discovery/ (service discovery working without app mesh/proxy)
------
.We can use angular route resolver to pre load the data before component gets rendered
2.For components that are lazy loaded(means it is only loaded into memory when we need it), we add CommonModule, Router.ForChild(childroutes) inside imports property, rather than BrowserModule and Router.ForRoute as we do inside app module, we can achieve this using dynamic import
3.We can use <ng-content></ng-content> to get the child component wrappend within the parent component, it is similar to react's this.children
4.we can also use <ng-content selector="seleect by class, select by attribute"> to display only selected component for example - 

child component - 
<div class="section1">
 <p>section 1</p> 
<div>
<div class="section2">
 <p>section 2</p> 
<div>

Then inside parent component 
<parent>
 <child></child>
</parent>

parent component - 
 <div>
	<h5> Parent component /<h5>
	<div *ngIf="somecondition">
          <ng-content selector=".section1"></ng-content>
	</div> 
        <div *ngIf="someothercondition">
          <ng-content selector=".section2"></ng-content>
	</div> 
 </div>

This will display different parts of child component based on somecontion using any kind of selector, it is called content projection
5.We can InjectionToken rather registering a class for DI
6.We can use #whatevername to reference a component and then using @ViewChild to get the actual dom element
For example
 <div class="trash" #mycomponent>
	<h5> Parent component /<h5>
 </div>

@ViewChild('#mycomponent') myreference: ElementRef;
const div = this.myreference.nativeElement; 

To get dom elements of the child comoponent as from step 4 we have to use @ViewContent
For component to component without parent child relationship use subject or behaviorSubject, the only difference among the two is that we can set initial values in behaviorSubject but not in subject

reference videos for rxjs operators - https://www.youtube.com/watch?v=ET2UPbsgPL8
ngOnChanges(),ngDoCheck() lifecycle hooks are similar to reacts pure component which also checks new props with the previous ones and re-renders the component only if they are changed 
angular hooks - (https://www.youtube.com/watch?v=qdvGXSUk7Mo)

# Use following operators on use case - 
retry(), retryWhen(), scan(), delay()  - making calls to api 
debounceTimer(), debounceUntilWhen() - when we wanna make an api call or fetch data directly after ceratin delay



(https://www.baeldung.com/linux/use-command-line-arguments-in-bash-script)

# Apache Kafka - sample project architecture
https://aws.amazon.com/blogs/developer/building-an-apache-kafka-data-processing-java-application-using-the-aws-cdk/ 
