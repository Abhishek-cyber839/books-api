1.Link to configure AWS RDS in private network and connect to it from outside of VPC - https://www.linkedin.com/pulse/connecting-rds-db-private-subnet-aws-from-using-ssm-port-srinivasan?trk=pulse-article_more-articles_related-content-card
follow option 3 for AWS RDS  Aurora postgres/mysql

2.JWT authentication with nginx as reverse proxy https://www.nginx.com/blog/authenticating-api-clients-jwt-nginx-plus/

3.Create a frontend service in public subnet with all the ec2 instances in public network, whilst put backend dotnet api cluster/services into the private network in the same vpc and then send requests
 from frontend(service1) to backend(service2) using service2's public ip address or we if there's a load balancer sitting in front of it then replace it with the ip address of service2's load balancer's ip address (https://aws.amazon.com/blogs/aws/amazon-ecs-service-discovery/) 

4.Service discovery tutorial - https://devops4solutions.medium.com/deploy-java-microservices-using-aws-ecs-service-discovery-27a8ba71dbd4

5.In service discovery we register services in route53 hosted zone, through which one service can make requests to another service 
with just the service name as route53 dns provider will resolve service name to the ip addresses of the running containers, so we don't even have to use load balancer in front of any service, other than if we're hosting multple ec2 instances
  

Unsplash api credentials - 
username - abhishek_abhishek
access_key - 8p0X14fHnptNmCnBUaS6uc0dpWFxrXoDM-DeDkcLB7c
secret_key - cbi3tQuEv62oarj2eAQslhf95avFSesQASiCYHsuutI
sample curl request - 
curl --location --request GET 'https://api.unsplash.com/search/photos?client_id=8p0X14fHnptNmCnBUaS6uc0dpWFxrXoDM-DeDkcLB7c&query=books'
replace the "query=<value>" with type of photos we want, and also iterate over all pages ?page=1 till "total_pages": <>,
sample response - 
{
 total: total number of images
 "total_pages": <>,
  result: []
}

documention - https://unsplash.com/documentation#search-photos
then write each file to folder and save it and upload it to s3 bucket as a zip file/ or directely write it to s3 bucket 


we can replace 
COPY ./start-up.sh /docker-entrypoint-initdb.d/
RUN chmod +x /docker-entrypoint-initdb.d/start-up.sh
with
COPY .init.sql /docker-entrypoint-initdb.d/
and then
GRANT ALL ON ALL TABLES IN SCHEMA public TO dbowner;
but the above didn't worked on line 8 of init.sql

we can follow the steps below to fix this
steps - 
1. go to postgres container
2. login as super user i.e pssql -U posgres <db-name>
3. run this command GRANT ALL ON ALL TABLES IN SCHEMA public TO <role-name>;
4. login as <user> to which the above role is attached to 
5. now you'll be able to perform crud operations

# inside init.sql
GRANT ALL PRIVILEGES ON DATABASE dotndockdb TO abhishek; 
ALTER DATABASE dotndockdb OWNER TO dbowner; 
the above two didn't worked as  
they will make this user temporary owner with only create access no read access on any table

Add these to dockerfile or pass in via docker run command if we don't wanna create a user and database from initial script
ENV POSTGRES_USER abhishek
ENV POSTGRES_PASSWORD docker
ENV POSTGRES_DB docker

Postgres commands 
\d - list table relations
\l - list tables
\zl tablename - verify access levels on table
\du - list all the users
\c databasename - connect to database
\q - exit of psql
ctrl + shift + L - clear psql console

# Add following lines to the dockerfile of the dotnet-core image, and also add a /health endpoint either in program.cs if using 
minimal api or a new controller, which will check the connection with database etc. 
On how to implement health check (https://code-maze.com/health-checks-aspnetcore/), (https://learn.microsoft.com/en-us/aspnet/core/host-and-deploy/health-checks?view=aspnetcore-6.0)
RUN apt-get update 
RUN apt-get --yes install curl
HEALTHCHECK --interval=5s --timeout=10s --retries=3 CMD curl --silent --fail http://localhost/health || exit 1
If we have multiple health endpoints we can use a script that makes multiple curl requests like below
COPY files/healthcheck.sh /healthcheck.sh
RUN chmod +x /healthcheck.sh
HEALTHCHECK --interval=60s --timeout=10s --start-period=10s \  
    CMD /healthcheck.sh || exit 1
That way you can do multiple requests via curl and let the script only return positive, if all requests succeeded.
we can use a sample script like this (https://stackoverflow.com/questions/37885025/rest-service-health-check-on-automatic-deployment)

We can utilise similar architecture for authentication/authorization using api gateway before we pass requests to the ECS cluster 
(https://aws.amazon.com/blogs/security/building-fine-grained-authorization-using-amazon-cognito-api-gateway-and-iam/)
(https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-the-id-token.html)
(https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-verifying-a-jwt.html)
(https://github.com/awslabs/aws-support-tools/tree/master/Cognito/decode-verify-jwt)
(https://aws.amazon.com/blogs/containers/securing-amazon-elastic-container-service-applications-using-application-load-balancer-and-amazon-cognito/)

# Approach for policy based auth for backend services using api-gateway, ecs and cognito 
1. Running Backend tasks in private subnet of vpc
2. Running Api gateway in public subnet of same VPC
3. Connect Api gateway using VPC link to the backend with network load balancer in place(or search if we can directly connect api gaetway to backend service in private subnet using vpc link without requiring NLB, as it is costly)
4. Set up Cognito user pools
5. When a user hits the the /get/pets endpoint it'll pass that request to lambda authorizer with the token from the cognito and then it'll get policy from dynamodb, returns it back to the api gateway which then caches that policy for the same user for further requests, and then pass that onto the network load balancer which'll then forward to the backend service.
6 The above steps assume that the user already exists in cognito user pool

# Approch for frontend auth using cognito, ecs, and ALB
1.A user sends a request to the application fronted by the ALB, which has a set of rules that it evaluates for all traffic to determine what action to carry out. The rule (such as the path-based rule saying all traffic for/login) when matched triggers the authentication action on the ALB. The ALB then inspects the user’s HTTP payload for an authentication cookie.
2.Because this is the user’s first visit, this cookie isn’t present. The ALB doesn’t see any cookie and redirects the user to the configured Amazon Cognito’s authorization endpoint.
3.The user is presented with an authentication page from Amazon Cognito, where the user inputs their credentials. Amazon Cognito redirects the user back to the ALB and passes an authorization code to the user in the redirect URL.
4.The load balancer takes this authorization code and makes a request to Amazon Cognito’s token endpoint.
5.Amazon Cognito validates the authorization code and presents the ALB with an ID and access token.
6.The ALB forwards the access token to Amazon Cognito’s user info endpoint.
7.Amazon Cognito’s user information endpoint presents the ALB with user claims.
8.The ALB redirects the user who is trying to access the application (step 1) to the same URL while inserting the authentication cookie in the redirect response.
9.The user makes the request to the ALB with the cookie and the ALB validates it and forwards the request to the ALB’s target. The 10.ALB inserts information (such as user claims, access token, and the subject field) into a set of X-AMZN-OIDC-* HTTP headers to the target.
11.The target generates a response and forwards to the ALB.
12.The ALB sends the response to the authenticated user.
When the user makes subsequent requests for HTTP request and response, the flow will go through steps 9–11. If the user makes a new request without the authentication cookie, it goes through steps 1–11.

# Use both fargate and ec2 launch type along with path based routing, we can basically move the entire architecure into private even the load balancer, and then create an api-gateway on public internet which will connect to the load balancer throught vpc link
(https://aws.amazon.com/blogs/containers/using-aws-application-load-balancer-path-based-routing-to-combine-amazon-ecs-launch-types/)
:- Do note that with fargate launch type we cannot access host instance's volume, i.e no persistence volume to overcome this we have to use EFS (https://aws.amazon.com/premiumsupport/knowledge-center/ecs-fargate-mount-efs-containers-tasks/) 
:- If we wanna access efs from our local pc we can create an ec2 in public subnet and efs in public/private depending on our need, then create a directory in efs and in ec2 for a user for ex - /efs/demo-user and mount that to efs, then use whatever directory you want in your local pc to mount to that /efs/local -> /efs/demo-user and make changes or access
 files using sshfs which gives us access(mounts local directories to remote servers) to the whole filesystem using ssh, the end solution will be 
/efs/local -> /efs/demo-user -> efs/efs
(https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh)

# We can use codebuild to run unit tests and report failure/success to github's api which will then compare the status of the current commit and based on that it will build the image and push it to ECR, change the task definition, make the service to deploy new containers in the ecs cluster, in the below example we can replace it with Xunit, moq tests of dotnet 
(https://aws.amazon.com/blogs/containers/create-a-ci-cd-pipeline-for-amazon-ecs-with-github-actions-and-aws-codebuild-tests/)
:- As dotnet 6 is not yet supported with codebuild so we can test it in github action using one 
(https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-net)
(https://docs.github.com/en/actions/deployment/deploying-to-your-cloud-provider/deploying-to-amazon-elastic-container-service)
(https://docs.github.com/en/actions/using-workflows/storing-workflow-data-as-artifacts) - to chek for the failure tests, we can also upload artifacts to s3 bucket
:- Make these changes in the actions.yaml file that we'll generate from the above 3 links
 - name: Build app
    id: build
    run: |
      <build command>

 - name: Run tests only when build passes
   id: test
   run: |
      <test command>

 - name: Build , tag and push new image to ecr only when test passes
   if: steps.test.outcome == 'success'

:- As from dotnet 6 onwards program.cs file is internal means we cannot acces it outside of the project so add the below to the .csproj file in order to access this in the test project
 Make the Program class public using a partial class declaration:
 public partial class Program { } and configure everything inside it
 <ItemGroup>
  <InternalsVisibleTo Include="$test_project_name" />
  </ItemGroup>
  later inside the test project within the test class add the following line 
  var webAppFactory = new WebApplicationFactory<Program>(); 
  as web application factory will run the api itself we don't have to use dotnet run <api_directory> in order to test our web api

  we can register our own in memory test database by creating out CustomWebApplicationFactory like below 
  public class CustomWebApplicationFactory<T>
    : WebApplicationFactory<TStartup> where T: class 
  with older versions we use to pass startup so we can also use the above
  services.AddDbContext<ApplicationDbContext>(options =>
   {
                options.UseInMemoryDatabase("InMemoryDbForTesting");
   });
   
   reference - https://dotnetthoughts.net/dotnet-minimal-api-integration-testing/

:- To share same instance of class among different test classes use ICollectionFixture<DatabaseFixture>
https://xunit.net/docs/shared-context

:- We can use postgres container during the testing step as test database of the build job which is provided by the github actions for ex - 
in action.yaml file add the below content
env:
  POSTGRES_HOST: localhost
  POSTGRES_USER: postgres
  POSTGRES_PASSWORD: postgres
  POSTGRES_DB: <db_name>

jobs :
  build:
    runs-on: ubuntu-latest   
  # Service containers to run with `container-job`
    services:
      # Label used to access the service container
      postgres:
        # Docker Hub image
        image: postgres
        # Provide the password for postgres
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: <db_name>
        ports:
          # Maps tcp port 5432 on service container to the host
          - 5432:5432  
        # Set health checks to wait until postgres has started
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
        - name: Install PostgreSQL client
          run: |
          apt-get update
          apt-get install --yes postgresql-client
        # queries database with postgres client
        - name: Query database
          run: psql -h postgres -d <db_name> -U postgres -c 'SELECT 1;'
          env:
            POSTGRES_USER: postgres
            POSTGRES_PASSWORD: postgres
        - name: Setup database
          id: database_setup
          run: psql -h postgres -U postgres_user -c 'CREATE DATABASE "<db_name>";'
          env:
            POSTGRES_USER: postgres
            POSTGRES_PASSWORD: postgres
        # only build and test if the database is successfully set up for use
        - name: Build app
          if: steps.database_setup.outcome == 'success'
          id: build
          run: |
              <build command>
        - name: Run tests only when build passes # use above env variable as connectionstring for entityframework test 
          id: test
          run: |
              <test command>
        # only build new image when test passes for new changes      
        - name: Build , tag and push new image to ecr only when test passes
          if: steps.test.outcome == 'success' 

reference to create database for postgres container - 

If the above database_setup step fails we can use the below to run migration script to create and seed database and then replace 
the properties in the above action.yaml file and Xunit-EFcore.txt, and we can also remove all the seeding logic from TestDatabaseFixture()
as database will already be seeded and verified from the below steps
- name: Run migrations
  run: psql -f /books-api/postgres-script/init.sql postgresql://postgres:postgres@localhost:5432/postgres
- name: Query database to verify above script
  run: psql -h postgres -d <db_name> -U postgres -c 'SELECT 1;'
  env:
    POSTGRES_USER: postgres
    POSTGRES_PASSWORD: postgres
reference - https://catzkorn.dev/blog/postgres-github-actions/      
     

Then inside Test Project Create new folder and files and seperate content inside Xunit-EFcore.txt to seprate files, as it shows us
how we can use postgres database to test against production like environment rather than in memory database

# BUT FOR REPOSITORY PATTERNS MOCKING IS BEST PRACTICE RATHER THAN USING IN-MEMORY DATABASE, IT SHOULD ONLY BE USED IF WE'RE ACCESSING ENTITY FRAMEWORK'S CONTEXT OBJECT DIRECTLY FOR EX - _dbcontext.dbtable.getAsync() INSIDE THE REPOSITORY INSIDE THE CONTROLLER RATHER THAN CALLING A _myRepository.getAsync()

-------
# Monitoring application logs from ECS containers using datadog
 - https://docs.datadoghq.com/containers/amazon_ecs/?tab=awscli 
 - https://docs.datadoghq.com/containers/amazon_ecs/data_collected/
 - https://docs.datadoghq.com/infrastructure/containermap/
 - https://docs.datadoghq.com/continuous_integration/pipelines/github/
 - https://docs.datadoghq.com/integrations/ecs_fargate/?tab=webui
 - https://docs.datadoghq.com/tracing/glossary/
 - https://docs.datadoghq.com/tracing/trace_collection/dd_libraries/dotnet-core/?tab=linux
 
:- We can use database monitoring of datadog for testing postgres container in the ci/cd pipeline 
:- communication from vpc to datadog vpc over private link https://docs.datadoghq.com/agent/guide/private-link/?tab=useast1

# ECS fargate type monitoring using datadog
  - https://www.datadoghq.com/blog/aws-fargate-monitoring-with-datadog/
  - https://www.datadoghq.com/blog/aws-fargate-monitoring-with-datadog/#explore-all-your-ecs-and-eks-fargate-monitoring-data ( ECS     logs into Datadog using Fluent Bit)
  - https://www.datadoghq.com/blog/aws-fargate-monitoring-with-datadog/#the-container-map (birdeye view of containers)
  - https://www.datadoghq.com/blog/asp-dotnet-core-monitoring/ (dotnet with datadog locally)
  - https://www.datadoghq.com/blog/deploy-dotnet-core-aws-fargate/ (dotnet with datadog on ecs fargate launch type)
  - https://www.datadoghq.com/blog/monitoring-ecs-with-datadog/#from-ecs-to-datadog

# Installing data-dog agent
 1.DD_API_KEY=7dffb9eba72d4e1c514884ed2e3c7d44
 2.DD_SITE="us5.datadoghq.com" 
 3.bash -c "$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script_agent7.sh)"
 - If the Agent is not already installed on your machine and you don't want it to start automatically after the installation, just     prepend DD_INSTALL_ONLY=true to the above script before running it.

# Monitoring AWS RDS Aurora Postgres
 - https://docs.datadoghq.com/database_monitoring/setup_postgres/aurora/?tab=docker
 (host your rds in private subnet then follow the verification process of above link,  through an EC2 instance which you'll create
  in public subnet of the same VPC, as we can't connect from our local i.e outside of VPC to our rds, so we have to ssh into the   public ec2 and then connect through it to the rds, or we can also you session manager/SSM for this purpose)
 - https://www.datadoghq.com/blog/monitor-aurora-using-datadog/

# Monitoring GitHub Actions workflow
 - https://www.datadoghq.com/blog/datadog-github-actions-ci-visibility/

# AWS App Mesh 
 - https://docs.aws.amazon.com/app-mesh/latest/userguide/what-is-app-mesh.html
 - https://docs.aws.amazon.com/app-mesh/latest/userguide/envoy.html
 - https://docs.aws.amazon.com/app-mesh/latest/userguide/sharing.html (for communications between ecs services in different      accounts/vpc's use shared mesh)
 - https://aws.amazon.com/blogs/aws/amazon-ecs-service-discovery/ (service discovery working without app mesh/proxy)

# We can create multiple VPC's in multiple regions under same account and the connect them using vpc-peering(inter-region), whilst 
  same applies to vpc's in the same region(intra-region), as long as their cidr blocks don't overlap, if a vpc has multiple cidr
  blocks, and one of them overlaps with it's peer then the request for peering will fail
  steps for vpc peering - 
  - go to peering connections tab, create a new vpc peering connection
  - in requester field add your vpc1, in accepter add your vpc 2, and their respective cidr blocks
  - then go to the account which has accepter vpc, if in same account but different region change region near to the profile, then      go to the peering connections tab and you'll see new connection request, accept or reject depending on what we wanna do
  - once accepted, change the route tables(main routing tables where igw is configured) of both the vpc's and add cidr block in     destination and target is peering connection, for example, in vpc 1 main route table add cidr block of vpc 2 in destion field     and peering connection in the target field, similar with vpc2, so the traffic will flow through peering connection
  - communication in vpc peering happens through private ips as they are part of similar network
  - by default a vpc can make 50 vpc connections with other vpc regardless of regions
  - by default we can have 25 connections in pending state
  - by deafult a pending connection can stay pending for 1 week
  - it is transitive, means if vpc1 -- vpc2 and vpc2 -- vpc3, then that doesn't mean vpc1 -- vpc3, if we wanna connect to vpc3     resources we can ssh into resources in vpc2 and then from their into vpc3, or we have to create a new vpc peering connection
    between vpc1 -- vpc3
  - we can have more than one private ip of our ec2 instances, and cannot change the existing one for the ec2 that is stopped or     running, best practice is to add EIP's to only the public facing ec2's 

# transit gateway VPC - 
  - To overcome the transitive property of the vpc peeering we use transit gateway vpc in a hub and source manner where we create a     new transit vpc and attach a vpn gateway to all other vpc's(a vpn gateway is required even if we are connecting from on premise     to our vpc's via a vpn)
  - This new transit vpc will have ec2 instances with cisco software running on them, and it is our responsibilty to manage those     instances, once setup we create a vpn connection between the vpc's and the transit vpc, now every single vpc will be able to     communicate with each other via this transit vpc without needing to manage n number of connections
  - For example - if their are 10 vpc's even in same or different regions we'll need to create n-1 i.e 9 connections between other     vpc's to communicate with each other, where as here we only make one connection with transit vpc and from each of the vpc

# transit gateway -
  - As we know that we have to manage our own ec2 and running 3rd party softwares on them to make a vpn connection becomes hassle,     to overcome this we use transit gateway, as it is self managed and no need for an extra vpc, ec2's 
  - we can even connect our on-prem resources through transit gateway
  - to connect on-prem to multiple vpc's in the same region, if we have vpc's and different regions we've to create either VPG or TGW for example - 
  if we have only one vpc in region 1 and one vpc in region 2 we can connect them through VGW as to minimize cost, but if we have 5 vpc's in region 1 and 5 vpc's in region 2, we can connect them using 2 seprate tgw's in different regions, then tgw1 <-> tgw2 communication

# Centrailsed Egress VPC - 
  - Egress is basically setting out outbound rules for our vpc, as we know that when we create a vpc we attach an igw for internet communication and ngw or nat instance(for lower bandwidth and traffic) for private resources to talk(oubound only) to internet, imagine if we have 10 vpc's we'll have to pay for 10 igw's and 10 ngw's, which will be expensive
  - To overcome above we won't create any internet gateway's or nat gateways or nat instances or vpc endpoints but rather create a
    another vpc, so now we will have 11 vpc's, connect those 10 vpc' to transit gateway, then change the route tables so that ll the traffic(public and private) will go to through transit gateway, now change the routing table of the TGW, to transfer all the traffic to the newly created VPC with one IGW and NGW, here it will first go to the NGW, then to the IGW, we can have multiple NGW or nat instances depending on traffic and then divide the traffic in the routing table of the TGW, such that half of the cidr block's ip's will go to NGW-one and half to the other placed in different availibility zones and both of them will forward traffic to single IGW
  - With the above solution they will not only have vpc peering but also eliminate the cost of having n number of nat gateways, instances, vpc endpoints and internet gateways
  - To add traffic filtering we can replace NGW or nat instance with an EC2 instance having firewall rules enables, but we'll need to manage it
  - We can connect more than one transit gateway from different regions depending  on the use case

  VP1
     \                        _Egress VPC
      \                      |           | 
  VP2 -- Transit gateway --- | ngw  ->  igw -->  External internet
       /                     |           | 
      /                       -----------
 VPC3/


# Site-to-Site VPN (https://www.youtube.com/watch?v=d_KKt5Q2aTQ) -
 - It is just a vpn connection created between on premises and aws vpc's, it uses virtual private gateways or transit gateway and
   customer gateway(a device or software installed on on-prem with multiple secure tunnels, if one failsit will pass traffic from another one) to create a vpn connection 
 - if we have one vpc and want to connect that to on premise we can use virtual private gateway instead of transit gateway, else with multiple vpc's its better to use transit gateway which will not only reduce cost and management but also enable vpc peering and vpn connection to on-prem resources, in this way we can access all the vpc's from on-prem and they can communicate with each other too
 - To make on prem user experience faster we can use the AWS global accelator which is responsible for finding out the nearest edge location(router) to route the traffic to vpc's, it is called accelerated site-to-site vpn
 - Firstly create a customer gateway in vpc sidebar, then a virtual private gateway or transit gateway, then go to s2s connections
   in the sidebar and add details of your cutomer gateway and virtual private gateways or transit gateway

# NACL vs Security groups (https://www.youtube.com/watch?v=p0XCg5VhKQA) - 
 - Network access control lists are the set of rules which work on subnet level, whilst security groups works on an instance/resource level, means the rules set in NACL applies to all the resources within the subnet whilst the rules set in security group applies to the resource which it is attached to.
 - one NACL can be applied to many subnets, whilst one subnet cannot have many NACL's
 - it works according to the rule numbers, lower the number higher the precendence
 - it uses empheral ports, the ports that are used by our operating system  and are open for only ceratin period of time just to
   make request and get response, for example - when we make an https request, firstly the host chooses the random port between 32700-65500 and forwards that to 443 of the server, and when the server sends the request it also does the same but at that time we'are the one who listens on 443, the sample request/response will look something like
 
 Request -
  client Port - any random port(32700-65500)
  client ip - ip belongs to client
  server port - 443
  server ip - ip belongs to server

 Response - 
  server port - any random port(32700-65500)
  server ip - ip belongs to server
  client Port - 443
  client ip - ip belongs to client

 so the NACL for client and server will be as follows - 
  outbound -
    allow custom port range 32700-65500
  inbound - 
    allow https port 443

# Google books-api 
 - https://developers.google.com/books/docs/v1/getting_started
 - we can use the above to populate the rds database

# Direct Connect (https://www.youtube.com/watch?v=-IeVqABxIVU)-
 - to connect on-premise to aws vpc's  privately, whilst with s2s vpn it was flowing through the internet
 - to reduce network costs, and increase bandwidth
 - to configure this we need a direct connect location, then we apply for a connection, once we get letter of authorization from 
   aws, we then give it to AWS APN(amazon partner networks) to install ethernet cables between on-prem router and direct connect, once    that's done we can congifure our vrtual interfaces(ip's) and can connect securely
 - it must be used only when we have to transfer large data set privately to the cloud, as it is a direct connection between vpc's and on-prem services whilst for other solutions request firstly goes to the ISP then to the cloud network, where we can face network congestion, so basically it bypasses the ISP,
 - when we have real time data-feed, for example playing a video on confrence, and don't wanna be part of network congestion
 - it has two connection types dedicated and hosted, where dedicated is between on-prem router and direct connect location, whilst hosted network is through amazon direct connect partners between on-prem router and direct connect location, dedicated connect is faster, we cannot change bandwidth in either after connection request is approved
 - to connect multiple vpc's from different regions, use direct conenct gateway, as mutiple virtual private gateways or transit gateways will then connect to direct conenct gateway, and it will connect to direct conenct location in one of the regions and your on-prem will be connected to that direct conenct location

# Private Link (https://www.youtube.com/watch?v=0bHXWIM4_0o) - 
 - to connect on-prem or vpc to vpc communication(in same or different account/region) or to SaaS hosted on aws without needing to manage IGW or vpc peering, and to connect privately
 - need 2 things vpc endpoint and vpc endpoint service
 - the producer creates an endpoint service using a network load balancer type and associates a private DNS name, consumer creates a vpc endpoint
 - the elastice interface of vpc endpoint has private ip which then connects to the private ip of vpc endpoint service thorugh its DNS name which forwards traffic to network load balancer(manadatory)
 - not suitable for more than 2 vpc peering as we'll need to create network load balancers and endpoints in each of the vpc's which will be expensive plus hard to manage
 - first create a network load balancer in the vpc we want to act as a producer, make it internal or external depending on whether we wanna associate private address with it, add target group instances to it
 - go to vpc service sidebar(Producer)
    >  endpoint services 
    > create new 
    > choose type as NLB 
    > associate the one we created above 
    > click on require acceptance so that every time someone requests a new connection we have to manually select it(only trusted ones)
    > associate a private DNS name if we have one
    > add tags(optional) , create
 - go to vpc service sidebar(Consumer)
   > endpoints
   > create new
   > servie type 
   > add the arn of the above service
   > choose that one, if it is available in multiple regions choose one
   > vpc where we wanna create this endpoint
   > private subnet in which it will create an ENI
   > security groups, make sure inbound/outbound rules can accept connections from the endpoint service
   > add tags(optional) , create
  - it'll then send request to the producer go to the endpoint services tab and accept/reject the connection
  - to verify whether an endpoint is created or not in consumer service
    > go to ec2 service
    > network interfaces
    > copy the arn of the vpc endpoint created above
    > search for this vpc endpoint arn
    > if you'll see an entry of type ENI created with an id of above vpc endpoint has private ip address associated with it
  - now we can send traffic privately

# ENI -
 - elastic network interface is similar to NIC on out machines
 - it is responsible for assigning ip address(private and public) and MAC address(12-bit)(0-5 represents manfacturer id, 6-11 represents serial number with the help of which we can trace the actual location though it can be changed with the help of online tools) like NIC
 - whenever we attach security group to something it is actually applied to its ENI not the actual reource
 - it gets created in the same subnet where the resource is present
 - we cannot detach it as long as the resource lives, reagrdless of whether it is stopped or running
 - resources like EC2, Load balancers, vpc endpoint etc..

# EBS vs instace based storage(https://www.youtube.com/watch?v=gJD8M5dcs4s)

-------
# Monitoring application logs from ECS containers using datadog
 - https://docs.datadoghq.com/containers/amazon_ecs/?tab=awscli 
 - https://docs.datadoghq.com/containers/amazon_ecs/data_collected/
 - https://docs.datadoghq.com/infrastructure/containermap/
 - https://docs.datadoghq.com/continuous_integration/pipelines/github/
 - https://docs.datadoghq.com/integrations/ecs_fargate/?tab=webui
 - https://docs.datadoghq.com/tracing/glossary/
 - https://docs.datadoghq.com/tracing/trace_collection/dd_libraries/dotnet-core/?tab=linux
 
:- We can use database monitoring of datadog for testing postgres container in the ci/cd pipeline 
:- communication from vpc to datadog vpc over private link https://docs.datadoghq.com/agent/guide/private-link/?tab=useast1

# ECS fargate type monitoring using datadog
  - https://www.datadoghq.com/blog/aws-fargate-monitoring-with-datadog/
  - https://www.datadoghq.com/blog/aws-fargate-monitoring-with-datadog/#explore-all-your-ecs-and-eks-fargate-monitoring-data ( ECS     logs into Datadog using Fluent Bit)
  - https://www.datadoghq.com/blog/aws-fargate-monitoring-with-datadog/#the-container-map (birdeye view of containers)
  - https://www.datadoghq.com/blog/asp-dotnet-core-monitoring/ (dotnet with datadog locally)
  - https://www.datadoghq.com/blog/deploy-dotnet-core-aws-fargate/ (dotnet with datadog on ecs fargate launch type)
  - https://www.datadoghq.com/blog/monitoring-ecs-with-datadog/#from-ecs-to-datadog

# Installing data-dog agent
 1.DD_API_KEY=7dffb9eba72d4e1c514884ed2e3c7d44
 2.DD_SITE="us5.datadoghq.com" 
 3.bash -c "$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script_agent7.sh)"
 - If the Agent is not already installed on your machine and you don't want it to start automatically after the installation, just     prepend DD_INSTALL_ONLY=true to the above script before running it.

# Monitoring AWS RDS Aurora Postgres
 - https://docs.datadoghq.com/database_monitoring/setup_postgres/aurora/?tab=docker
 (host your rds in private subnet then follow the verification process of above link,  through an EC2 instance which you'll create
  in public subnet of the same VPC, as we can't connect from our local i.e outside of VPC to our rds, so we have to ssh into the   public ec2 and then connect through it to the rds, or we can also you session manager/SSM for this purpose)
 - https://www.datadoghq.com/blog/monitor-aurora-using-datadog/

# Monitoring GitHub Actions workflow
 - https://www.datadoghq.com/blog/datadog-github-actions-ci-visibility/

# AWS App Mesh 
 - https://docs.aws.amazon.com/app-mesh/latest/userguide/what-is-app-mesh.html
 - https://docs.aws.amazon.com/app-mesh/latest/userguide/envoy.html
 - https://docs.aws.amazon.com/app-mesh/latest/userguide/sharing.html (for communications between ecs services in different      accounts/vpc's use shared mesh)
 - https://aws.amazon.com/blogs/aws/amazon-ecs-service-discovery/ (service discovery working without app mesh/proxy)

# AWS App Runner
 - we can use app runner to run containerised applications that are only served over http/https
 - we can either provide ECR or gihub connection so that it creates an image whenever we merge new changes into the branch provided in the build configuration
 - when it creates a container it generates a service url using which we can access our app
 - we can also give it minimum number of containers called app instances to avoid cold start
 - it supports concurrency, auto deployment based on whether we merge new changes or push new image to ECR

------
.We can use angular route resolver to pre load the data before component gets rendered
2.For components that are lazy loaded(means it is only loaded into memory when we need it), we add CommonModule, Router.ForChild(childroutes) inside imports property, rather than BrowserModule and Router.ForRoute as we do inside app module, we can achieve this using dynamic import
3.We can use <ng-content></ng-content> to get the child component wrappend within the parent component, it is similar to react's this.children
4.we can also use <ng-content selector="seleect by class, select by attribute"> to display only selected component for example - 

child component - 
<div class="section1">
 <p>section 1</p> 
<div>
<div class="section2">
 <p>section 2</p> 
<div>

Then inside parent component 
<parent>
 <child></child>
</parent>

parent component - 
 <div>
	<h5> Parent component /<h5>
	<div *ngIf="somecondition">
          <ng-content selector=".section1"></ng-content>
	</div> 
        <div *ngIf="someothercondition">
          <ng-content selector=".section2"></ng-content>
	</div> 
 </div>

This will display different parts of child component based on somecontion using any kind of selector, it is called content projection
5.We can InjectionToken rather registering a class for DI
6.We can use #whatevername to reference a component and then using @ViewChild to get the actual dom element
For example
 <div class="trash" #mycomponent>
	<h5> Parent component /<h5>
 </div>

@ViewChild('#mycomponent') myreference: ElementRef;
const div = this.myreference.nativeElement; 

To get dom elements of the child comoponent as from step 4 we have to use @ViewContent
For component to component without parent child relationship use subject or behaviorSubject, the only difference among the two is that we can set initial values in behaviorSubject but not in subject

reference videos for rxjs operators - https://www.youtube.com/watch?v=ET2UPbsgPL8
ngOnChanges(),ngDoCheck() lifecycle hooks are similar to reacts pure component which also checks new props with the previous ones and re-renders the component only if they are changed 
angular hooks - (https://www.youtube.com/watch?v=qdvGXSUk7Mo)

# Use following operators on use case - 
retry(), retryWhen(), scan(), delay()  - making calls to api 
debounceTimer(), debounceUntilWhen() - when we wanna make an api call or fetch data directly after ceratin delay



(https://www.baeldung.com/linux/use-command-line-arguments-in-bash-script)

# Apache Kafka - sample project architecture
https://aws.amazon.com/blogs/developer/building-an-apache-kafka-data-processing-java-application-using-the-aws-cdk/ 

# Advanced CSS 
https://www.youtube.com/watch?v=TUD1AWZVgQ8
 - custom-media
 - container
 - clamp 

 # react best practices 
 https://www.youtube.com/watch?v=GGo3MVBFr1A
 - AbortController with fetch for cancelling multiple asynchronous requests

 # rem vs em
 https://www.youtube.com/watch?v=H4UtKu11yXg
 - rem is relative(dependent font size) to the root(html) element
 for ex - html {font-size: 10px}, p{ font-size: 2rem} the p tag will have 20px
 - em is relative to the parent element

 # react do's and don'ts 
 https://www.youtube.com/watch?v=4FhJkX18fS8
 - use purefunctional components to avoid unnecessary re-rendering, only do it when the props are changed
 - UseCallback 

 # react mistakes
 https://www.youtube.com/watch?v=E1cklb4aeXA

# react bug fix 
https://www.youtube.com/watch?v=XUwzASyHr4Q

# CSRF tokens
https://www.youtube.com/watch?v=80S8h5hEwTY

# CSS :has,:is selectors Advanced usgaes
https://www.youtube.com/watch?v=K6CWjkDgQnE

# react swr !important
https://www.youtube.com/watch?v=6gb6oyO1Tyg
https://swr.vercel.app
- it used to preload data and better than useEffect, as it caches data and only makes new request whenever there's a change in 
  data like add, delete or update, but not make unnecessary requests like useEffect as it does even for a small state change
- it use optimistic UI to match the new data and expecting data to make new request or not

# mongodb refresh 
https://www.youtube.com/watch?v=ofme2o29ngU

# angular ngrx !important
https://www.youtube.com/watch?v=kx0VTgTtSBg

# Eventbridge vs SQS
 - if we have more than one consumer than wwe should try to use AWS Eventbridge as comapred to SQS
  because once the message is successfully processed in SQS it won't be available for other consumers but we can avoid this using
  SNS and creating a topic anc subscribing to that topic and using multiple Queues and lambdas(depending on use case) for each consumer
  but that'll be hard to manage and more cost, whereas with Eventbridge we have to push an event into one single event bus and then
  apply rules according to which it can send same event to more than one consumers, and also filter events based on rules, hence we 
  have to only manage only one single event-bus as compared to N number of Queues and the unprocessed messages will go into DLQ

# Deplying microservices to different environments in ECS or EKS
 - If we ever wanna deploy same docker images to 3 different environments dev,qa and prod for testing then we could use 3 different
   aws accounts each with one cluster and ecr registery, and then push that image to it,but if we wanna avoid averhead then we
   could create 3 different clusters each with its own load balancer and use one ecr registery and then first deply to lets say 
   dev-cluster then test it and later to qa-cluster, similarly to prod-cluster, we can invoke microservices in those different clusters
   using their corresponding load balancers, same can be done with EKS

# Step functions workflow in backend with Eventbridge
  - The frontend app will push events to Eventbridge event bus, later based on the rules we can call specific step-functions workflow
    ,where it'll use api gateway, vpc link to call the microservices hosted in ECS or EKS, and perform other tasks as seperate branches

# AWS systems manager -
 It's a service used to monitor on prem servers and ec2 intances along with other aws services such as s3 bucket, dynamodb, rds intances
 It monitors those services/intances for regular updates or faults and installs them on scheduled bases, for ex - it will monitor all the
 servers in patch group and install the newer versions of those softwares after scanning velnerabilities on its own, or lets say if an rds
 instance goes down it will restart that, or lets say someone makes a s3 bucket access public from prviate then it will change that back to 
 private, few terms to remember
 patch baseline - list of softwares/services we wanna monitor/scan regularly
 patch group - collection of servers grouped by environment for ex - dev,qa, prod
 compliance report - after installing the required updates it will generate a compliance report and push that to s3 bucket, that compliance
 report will tell us which services or servers have what we want them to have and which of them don't, for ex - i don't want my s3 buckets
 to be public, but someone within a company changes that to public without notifying anyone and lets say that s3 bucket is monitored by
 systems manager, then it will revert that back to private
 run command - this command is responsible for restarting the servers, installing new updates
 If we wanna mointor on prem servers then we have to install ssm argent on them
 resources - 
 https://aws.amazon.com/blogs/aws/category/management-tools/amazon-ec2-systems-manager/
 https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-instances-and-nodes.html
 https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html
 https://github.com/awslabs/aws-systems-manager/tree/master/Documents/Automation - !important

 # Cloudwatch 
  - we can add events that will run on specific time for ex - AWS systems manager to install new updates weekly on a specific day
  - we can set an alarm for biiling i.e - if our bill exceeds more than the set amount it will trigger a almbda or send an email

# Cloudtrail
  - it not only collects logs like cloud watch but also records api endpoints for different services for ex -  a new user added
     to the group, permissions removed from the user, we changed configurations of ec2 instance etc. 

# AWS Kinesis -
 - it is of two 3 types stream, firehose and analytics
 - in stream type we can push events/data to Kinesis firehose and then consumers can consume that data and later they can store that data
   where ever they want as persistence storage
 - in firehose type we cannot consume that data as a consumer application but rather the data is directly stored into persistence
   storage, currently there are 4 of them s3 buckets, opeansearch (elastic search), redshift, we can only have one lambda function 
   which will get executed on new data, this can remove unwanted properties or add new properties in the data, and later that date
   is pushed to s3 bucket and is partioned on year/month/day folder for ex - backend app will send ip address of the client to kinesis firehose
   {
     "Ip" : "asd.sdf.asd.awq"
   }
   later using lambda transformer we can call an external api and get details like location or other metadata
   {
    "Ip" : "asd.sdf.asd.awq",
    "city" : "adasdfasd"
    "ppostcode" : "asdasd"
    "timestamp" : "adsas" 
   }
 - in case we also wanna have the orginal data we can also store that in seperate s3 bucket for later use
 - we can have transformations that will get executed once the data is pushed to kinesis firehose, which will convert data to parquet files using aws glue
 - data is pushed or lambda transformer is executed(if we plan to use one) based on batch interval or batch capacity which ever comes 
   first
 - with streams the consumers will consume data through sharding(a database scheme where data is partioned over several serveres based on column
   to reduce latency), it is of two types standard and extended, only use extended if we have more than 3 consumer applications 
 - in the end we can query data stored in s3 files using aws athena  

   - https://www.youtube.com/watch?v=UE34CWAhT3o

 # CQRS patter using lambda and AWS RDS - 
  we can also implement CQRS pattern using lambda by having seperate labdas for commands and one lambda for queries, and giving only
  read only access to the query lambda and read and write access to other three command lambdas, we can also do similar things if we are 
  using an api rather than lambdas, then just create two DbContext classes one for commands and one for query only, later in programs.cs
  inject the url of readonly database instance for query DbContext, and read & write access database instance url to commands DbContext
   for ex - 
    var connectionString = builder.Configuration.GetConnectionString("ReadOnlyInstanceconnectionstring");
    builder.Services.AddDbContext<QueryDbContext>(x => x.UseSqlServer(connectionString));

    var connectionString = builder.Configuration.GetConnectionString("ReadWriteInstanceconnectionstring");
    builder.Services.AddDbContext<CommandDbContext>(x => x.UseSqlServer(connectionString));
  This way we can have seperation of logic and we won't be able to perform unwanted operations on endpoints where we're not supposed to
 
 # AWS App Config - 
  - It is used to deploy configurations on different environments, configuration is any data related to our app that we wanna read during 
    runtime, for ex - 
    {
       customer_id : [1,2,3,4]
       showPreview : true
       isEnabled : false
       .....
    }
    which we can later use in our app/microservices at later point
  - it is similar to aws parameter store except that it can also perform systax validation using json schema validator or lambda
     function to avoid errors when reading configuration in service/application
  - configuration profile is a file containing configuration data that we wanna use, it could be in text, yaml ,json format
  - we can attach it to cloud watch alarm , and it will rollback the newer changes whenever that alarm will trigger, for ex - 
    someone has made changes to the config file and have deployed that on prod env, then while reading that configuration file an
    error has happened and cloudwatch it monitoring that error and we have set an error lets say a 500 error, once that error will be thrown
    it will trigger the cloud watch alaram and that alarm will make the app config to rollback to previous revision
  - it is a pull based mechanism, i.e whenever newer changes are deployed the app using that configuration profile(configuration file/data)
     will not get notified on its own, but rather we have to make an api call periodically to get newer configurations
  - if we are using app config with lambda  then we can utilize lambda layers extension, which will make api calls on its own for 
    newer configuration (see - how to use app config with lambda)
  - configuration profiles are of tow type - feature flag and freeform, in freeform we can validate the file using validators discussed
    above whilst in feature flag we can't and it is for short term configuration data
  - deployment strategy is how we wanna delpoy newer configuration changes, its not a good practice to use ALL_AT_ONCE type as it will
    deploy all the changes at once, either deploy changes at an exponential rate or set custom percentage
  - with ecs we can use appconfig as a sidecar(another container instance attached to main container) which will call the app config
    api and will give the newer configurations to our microservice container for use

    # AWS dynamodb -
    - it is a managed(backups available by default, cloudwatch monitoring) nosql database, we can scale(replicated on its own) it globally whenever required
    - items are stored in json format upto max of 400kb per item
    - it offers a flexibility of caching using DAX(dynamodb accelator) for frequently retrieved data
    - it offers lowere latency as compared to other aws offer nosql solutions
    
    # AWS document db - 
    -  it is a managed(backups available by default, cloudwatch monitoring) nosql database, it can have max of 15 read replicas globally
    - there's no limit on size of max item size
    - it useful if we wanna store complex/nested json objects for ex - 
      {
         prop1 : value
         prop2 : {
          prop1 : value
          prop2 : {
              prop1 : value
              prop2 : {
                
              }
           } 
         }
      }
      - we can use mongodb sdk to query and store data
      - we can also validate data using json validator (please see - how to validate json data mongodb)
      - https://www.youtube.com/watch?v=fYlZ8_2MPxE (!important video skip till 8 minutes)
      - in normal database replication we copy all 5 layers, (api, query, caching, logging, storage), but with documentdb we're only
        replicating storage layer(logging & storage) during replication process in different AZ's, for computaion layer((api, query, caching) 
        it'll be one instance and later we can scale that one instance or add more instances and make them read replicas
      - it backups data into s3 buckets, data gets partioned into 10gb chunks
      - migration for on prem or on cloud database to documentdb using AWS DMS (database migration system), 
        choose "migrate current and ongoing changes" options to also copy the new data as well (https://www.youtube.com/watch?v=Png6NIV73qc) 

        # Why to use containers vs VMS ?
          - https://www.youtube.com/watch?v=eyNBf1sqdBQ
          - If we don't use containers then inorder to run multiple instances for same microservice w have to use multiple servers, for ex - 
            a node app listening on port 3000 can only run one instance on one server, if we try to run another one it'll throw port already
            bounded exception, to avoid this we can install virtual machines in one server, and deploy that node application on those VM's, but since
            vm's are an operating system in their own it'll require time to boot applications and it will require huge space installing several
            vm's
          - we can avoid the above issues using containers, as containers are not an operating system, they use the underlying operating
            system of the instance on which they are running, and since they only contain the image layers not the oprating system files(like VM's)
            it takes less time to restart if a container crashes to restart, and we can run as many instances of similar microservice with less
            space and memory, they are managed by docker engine, which is responsible for allocating hardware resources to those containers, like hypervisor
            is responsible for allocating reources to VM's
          - one disadvantage of using containers is that since they are running on host operating system, if an oprating system creashes they'll also 
            get destroyed
          - another disadvantage is that we cannot run linux based docker images on windows(unless we're using windows subsystem for linux),
            and windows images on linux, since both have different kernels, and for docker kernels is what is uses to run those containers
    
    # AWS CloudFront - 
     - It is a  CDN, where request from client first goes to nearest edge location(where caches are kept), if it doesn't finds any resources
     with that edge location then it passes request to the nearest regional edge location(where caches are kept for longer period of time)
     , if regional edge location doesn't have those resources then the request goes to the actual origin/server, then response it cached
     at both regional edge location & the nearest edge location to the client
     - The reason why we store ressources at regional edge location is so that if the nearest edge location to the client deletes
     it cache we can still retrieve cache resources from regional edge location rather than putting load on the actual server
     - We can use edge lambda functions to intercept requests & responses from CloudFront to client
     - We can use AWS WAF(web access firewall) to blacklist the ip addresses/regions from which we don't wanna recieve requests
     - we use third party geo location services to serve content only in specific parts of the world rather then entire world, countries
     or cities or postcodes that will be in whitelist will see the content remaining will see the custom error page
     - it uses AWS Shield(it protects from DDOS attacks)
     - we can manually update the old cached content with the new one


     # Important concepts - 
      - cluster role binding
      - cluster role
      - service account
      - if we're using path based routing in eks then we've to deploy ingress as an ALB(Frontent) or NLB(backend) and add routing rules to it,
         later CloudFront will send rquests to the ALB if it doesn't have content
      - if we're using path based routing in apps deployed on EC2 instances then we just have to go to load balancer edit it's rules and 
        add path based routing under listeners > protocols(http or https) > add path for which we want path based routing > select target group
      - as our services will get recreated when they are destroyed and thus ingress load balancer will get a new ip every time so 
       follow this to get the static ip address (https://stackoverflow.com/questions/60095864/how-can-i-assign-a-static-ip-to-my-eks-service)
       or we can put NLB infront of ALB as only NLB have static ip address by default

    # AWS Route 53 - 
     - Firstly we've to register a domain, id we don't have a domain we can buy one (click on create new)
     - after that a hosted zone(place where we'll have records) will be creatd automatically for this domain
     - later create a new record type 
        A - maps domain name to an ipv4 address
        AAA - maps domain name to an ipv6 address
        CNAME - maps another domain name to your domain
        Alias - maps your domain name to an AWS service's url for ex - NLB,ALB or an app depllyed on elastic beanstalk

    # AWS App Mesh -
    - it has 4 main concepts i.e virtual gateway, virtual router, virtual service, virtual node
    - virtual gateway - client sends requests to load balancer and it then forwards to virtual gateway, which uses envoy proxies(not as a sidecar)
      to forward traffic to virtual service, we can add ingress rules to filter out traffic at virtual gateway
    - virtual service - it sits in between virtual router and virtual gateway, it forwards traffic to virtual node, inside code we should
      be using whatever name of the virtual service we've given to communicate
    - virtual node - this what forwards traffic to real pod, we configure it through cloud map or dns resolution,
       for ex - when using dns it will resolve the host name to the ip address, lets say a backend service is talking to a rds database service
       and we're accessing that as rds-service inside code then when we'll use it will use it's host name(xx.aws.rds) and forward request to the database cluster
       , but if we choose to use cloud map then first we've to create a namespace and then inside that namespace we've to create a 
       service, and later add ip address's of ec2 instances on which pods will be running, if we're using fargate launch type then 
       an entry is already created inside the cloud map as aws manages the creation and destortion of ec2 instances in fargate launch tyoe
       so we can not get ip's of ec2 instances, but if choose managed cluster then we've to manually add ip addresses of the ec2 instances
    - virtual router - it sits between virtual service and virtual node, and routes traffic to appropriate version of that service,
      for ex - let say we've deployed a newer version of our app and we want 20% traffic to go to newer version and 80% to older version, to test the 
      newer changes then we can use weighted routing inside virtual router, otherwise it'll direct all traffic to single service,
      we don't have to use virtual router if we're only sending traffic to one version of the app
    - every container inside the pods contains envoy as a sidecar proxy for communication(i.e request and response flows the following way 
      (client <-> [x-ray <-> envoy <-> container]), and x-ray container for logging/tracing to aws xray
    - inside backend option of virtual node we've to add the name of the virtual service that we wanna communicate within the cluster
    - we can also use tls communication within services using aws certificate manager
    - If we ever wanna do canary deployment using app mesh follow these steps 
      1. do code changes, push them to github, build new image using github actions, then tag that image using v2, push
         that image to ECR
      2. create a brand new task definition and service and supply that new task definition
      3. if we're using managed node groups then go and add new entry in cloud map
      4. create new virtual node and virtual router, and then inside virtual router use weighted routing

  # Steps to use new image in ECS or EKS - 
   1. do code changes, push them to github, build new image using github actions, then tag that image using v2, push
         that image to ECR
   2. create a brand new task definition and service and supply that new task definition

    